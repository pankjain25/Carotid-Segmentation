{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79599119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "import PIL\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import datetime, os\n",
    "import matplotlib.pyplot as plt\n",
    "#from tensorflow.keras.layers import *\n",
    "#from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "#from tensorflow.keras.optimizers import Adam\n",
    "from IPython.display import clear_output\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1eed9248",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('UNet_Model_3_ICA.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42ba7841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(pred_mask: tf.Tensor) -> tf.Tensor:\n",
    "    \"\"\"Return a filter mask with the top 1 predictions\n",
    "    only.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pred_mask : tf.Tensor\n",
    "        A [IMG_SIZE, IMG_SIZE, N_CLASS] tensor. For each pixel we have\n",
    "        N_CLASS values (vector) which represents the probability of the pixel\n",
    "        being these classes. Example: A pixel with the vector [0.0, 0.0, 1.0]\n",
    "        has been predicted class 2 with a probability of 100%.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tf.Tensor\n",
    "        A [IMG_SIZE, IMG_SIZE, 1] mask with top 1 predictions\n",
    "        for each pixels.\n",
    "    \"\"\"\n",
    "    # pred_mask -> [IMG_SIZE, SIZE, N_CLASS]\n",
    "    # 1 prediction for each class but we want the highest score only\n",
    "    # so we use argmax\n",
    "    pred_mask = tf.argmax(pred_mask, axis=-1)\n",
    "    # pred_mask becomes [IMG_SIZE, IMG_SIZE]\n",
    "    # but matplotlib needs [IMG_SIZE, IMG_SIZE, 1]\n",
    "    pred_mask = tf.expand_dims(pred_mask, axis=-1)\n",
    "    return pred_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0da4b4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(input_image: tf.Tensor) -> tuple:\n",
    "    \"\"\"Rescale the pixel values of the images between 0.0 and 1.0\n",
    "    compared to [0,255] originally.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_image : tf.Tensor\n",
    "        Tensorflow tensor containing an image of size [SIZE,SIZE,3].\n",
    "    input_mask : tf.Tensor\n",
    "        Tensorflow tensor containing an annotation of size [SIZE,SIZE,1].\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        Normalized image and its annotation.\n",
    "    \"\"\"\n",
    "    input_image = tf.cast(input_image, tf.float32) / 255.0\n",
    "    #input_mask = tf.cast(input_mask, tf.float32) / 255.0\n",
    "    return input_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89384df4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "outPath = r'G:\\(11)-AI-in-MEDICINE-Journal\\(1)-UNet\\(1)-ICA\\Prediction-3'\n",
    "path = r'G:\\(11)-AI-in-MEDICINE-Journal\\(0)-Database\\(0)-ICA\\Training-Data\\val-3'\n",
    "\n",
    "    # iterate through the names of contents of the folder\n",
    "for image_path in os.listdir(path):\n",
    "\n",
    "     # create the full input path and read the file\n",
    "    input_path = os.path.join(path, image_path)\n",
    "    sample_image = plt.imread(input_path)\n",
    "    sample_image = normalize(sample_image)\n",
    "    # Predict the image\n",
    "    sample_image2 = tf.expand_dims(sample_image, axis=0)\n",
    "    # create full output path, 'example.jpg' \n",
    "     # becomes 'rotate_example.jpg', save the file to disk\n",
    "    fullpath = os.path.join(outPath, 'Pred_'+image_path)\n",
    "    prediction_inference = model.predict(sample_image2)\n",
    "    pred_mask = create_mask(prediction_inference)\n",
    "    pred_img = tf.keras.preprocessing.image.array_to_img(pred_mask[0])\n",
    "    plt.imshow(pred_img,cmap=plt.get_cmap('gray'), vmin=0, vmax=1)\n",
    "    plt.imsave(fullpath,pred_img,cmap=plt.get_cmap('gray'), vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51c4d5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[0]\n",
      "  [0]\n",
      "  [0]\n",
      "  ...\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]]\n",
      "\n",
      " [[0]\n",
      "  [0]\n",
      "  [0]\n",
      "  ...\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]]\n",
      "\n",
      " [[0]\n",
      "  [0]\n",
      "  [0]\n",
      "  ...\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0]\n",
      "  [0]\n",
      "  [0]\n",
      "  ...\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]]\n",
      "\n",
      " [[0]\n",
      "  [0]\n",
      "  [0]\n",
      "  ...\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]]\n",
      "\n",
      " [[0]\n",
      "  [0]\n",
      "  [0]\n",
      "  ...\n",
      "  [0]\n",
      "  [0]\n",
      "  [0]]], shape=(224, 224, 1), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(max(pred_mask))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
